这有一些**选择激活函数的经验法则**：

- 如果输出是0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择ReLU函数。
- ReLU是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用ReLU激活函数。
- 有时，也会使用tanh激活函数，但Relu的一个优点是：当 _z_ 是负值的时候，导数等于0。

![激活函数](https://camo.githubusercontent.com/bd9ec2e74dd2dd4ef2678ffbb89eecf7e0257d606d96468f92746326e75325f7/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f4c315f7765656b335f392e6a7067)

要让你的神经网络能够计算出有意义的函数，**必须使用非线性激活函数**

**初始化你的神经网络的权重，不要都是0，而是随机初始化**

有一条经验规律：**经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。**


# 构成深度神经网络的基本模块
1. 导包
2. 处理数据集
知道输入、输出（训练维度、训练批次）- 确定维度和形状
数据维度对齐-数据维度太高，也需要扁平化- Reshape
数据集居中和标准化-防止某些特征的占比太大或太小，从未影响优化-"Standardize" 
3. 构建神经网络
	1. 定义神经网络结构（输入单元 、隐藏单元 等）![[Pasted image 20241104210925.png]]
	2. 初始化模型的参数 （一定要注意维度对齐）![[Pasted image 20241104211239.png]]

	3. 循环： 
		  - 实现前向传播 -维度对齐
		  - 计算损失 
		  - 实现向后传播以获取梯度 -**会用就行，不用太清楚求导理论**
		  - 更新参数（梯度下降）
	5. 合并函数为model（）
4. 预测函数，查看效果
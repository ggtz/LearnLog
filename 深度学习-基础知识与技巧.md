这有一些**选择激活函数的经验法则**：

- 如果输出是0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择ReLU函数。
- ReLU是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用ReLU激活函数。
- 有时，也会使用tanh激活函数，但Relu的一个优点是：当 _z_ 是负值的时候，导数等于0。

![激活函数](https://camo.githubusercontent.com/bd9ec2e74dd2dd4ef2678ffbb89eecf7e0257d606d96468f92746326e75325f7/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f4c315f7765656b335f392e6a7067)

要让你的神经网络能够计算出有意义的函数，必须使用非线性激活函数

初始化你的神经网络的权重，不要都是0，而是随机初始化

有一条经验规律：**经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。**

[![神经网络Gradient Descent推导过程](https://github.com/loveunk/machine-learning-deep-learning-notes/raw/master/deep-learning/img/dl-basic-summary-of-gradient-descent.png)](https://github.com/loveunk/machine-learning-deep-learning-notes/blob/master/deep-learning/img/dl-basic-summary-of-gradient-descent.png)


### 前向传播和反向传播


之前我们学习了构成深度神经网络的基本模块，比如每一层都有前向传播以和反向传播步骤，这里我们讲讲如何实现这些步骤。

先讲前向传播，输入 _a[l-1]_ ，输出是 _a[l]_ ，缓存为 _z[l]_ ；从实现的角度来说我们可以缓存下 _w[l]_ 和 _b[l]_ ，这样更容易在不同的环节中调用函数。
[![](https://camo.githubusercontent.com/e7b7ee1b376cc35c3f52284a81c47fa03b4fdc7f6214b6de076930e354e1d27a/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f37636663346235666539346463643966653731333061633532373031666564352e706e67)](https://camo.githubusercontent.com/e7b7ee1b376cc35c3f52284a81c47fa03b4fdc7f6214b6de076930e354e1d27a/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f37636663346235666539346463643966653731333061633532373031666564352e706e67)

所以**前向传播的步骤可以写成** ：

- _z[l] = W[l] · a[l-1] + b[l]_
- _a[l] = g[l](z[l])_

向量化实现过程可以写成：

- _Z[l] = W[l] · A[l-1] + b[l]_
- _A[l] = g[l](Z[l])_

前向传播需要喂入 _A[0]_ 也就是 _X_ ，来初始化；初始化的是第一层的输入值。 _a[0]_ 对应于一个训练样本的输入特征，而 _A[0]_ 对应于一整个训练样本的输入特征，所以这就是这条链的第一个前向函数的输入，重复这个步骤就可以从左到右计算前向传播。

**反向传播的步骤：**

输入为 _da[l]_ ，输出为 _da[l-1]_ ， _dw[l]_ , _db[l]_

[![](https://camo.githubusercontent.com/db2fe519dd1272a6671bed59d9b1c1f812ad32c0f0e3550421438d13852c65b2/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f63313364326138666132353831323561353339383033306339373130316565312e706e67)](https://camo.githubusercontent.com/db2fe519dd1272a6671bed59d9b1c1f812ad32c0f0e3550421438d13852c65b2/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f63313364326138666132353831323561353339383033306339373130316565312e706e67)

所以反向传播的步骤可以写成：

（1） _dz[l] = da[l]*g[l]'(z[l])_

（2） _dw[l] = dz[l] · a[l-1]_

（3） _db[l] = dz[l]_

（4） _da[l-1] = w[l]T · dz[l]_

（5） _dz[l] = w[l+1]Tdz[l+1] · g[l]'(z[l])_

式子（5）由式子（4）带入式子（1）得到，前四个式子就可实现反向函数。

向量化实现过程可以写成：

（6） _dZ[l] = dA[l]*g[l]'(Z[l])_

（7） _dW[l] = (1/m)dZ[l] · A[l-1]T_

（8） _db[l] = (1/m)np.sum(dz[l],axis=1,keepdims=True)_

（9） _dA[l-1] = W[l]T.dZ[l]_

总结一下：

[![](https://camo.githubusercontent.com/59a51c6a646400b9111e331e462bfc8b3aca44047ab51296a2b4f4726d163591/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f35336135623463373163306661636663383134356166336235333466383538332e706e67)](https://camo.githubusercontent.com/59a51c6a646400b9111e331e462bfc8b3aca44047ab51296a2b4f4726d163591/68747470733a2f2f7261772e6769746875622e636f6d2f6c6f7665756e6b2f646565706c6561726e696e675f61695f626f6f6b732f6d61737465722f696d616765732f35336135623463373163306661636663383134356166336235333466383538332e706e67)

第一层你可能有一个**ReLU**激活函数，第二层为另一个**ReLU**激活函数，第三层可能是**Sigmoid**函数（如果做二分类的话），输出值为，用来计算损失；这样就可以向后迭代进行反向传播求导来求 _dw[3]_ ， _db[3]_ ， _dw[2]_ ， _db[2]_ ， _dw[1]_ ， _db[1]_ 。在计算的时候，缓存会把 _z[1]_ _z[2]_ _z[3]_ 传递过来，然后回传 _da[2]_ ， _da[1]_ ，可以用来计算 _da[0]_ ，但我们不会使用它，这里讲述了一个三层网络的前向和反向传播，还有一个细节没讲就是前向递归——用输入数据来初始化，那么反向递归（使用**Logistic**回归做二分类）——对 _A[l]_ 求导。